{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOdWNeMPqLo40QxMU35tG2v",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/s-strasberg/LinguaLens/blob/liliana-data-preprocessing/FinalDataCollectionCode_py.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def clean_text(text):\n",
        "# Lowercases all characters, removes punctuation, digits, and non-letter characters, preserves accented Latin characters and CJK (Chinese, Japanese, Korean) characters\n",
        "    start_idx = text.find(\"*** START OF\")\n",
        "    end_idx = text.find(\"*** END OF\")\n",
        "    if start_idx != -1 and end_idx != -1:\n",
        "        text = text[start_idx:end_idx]\n",
        "\n",
        "    # Remove all non-letter characters except spaces (including numbers, punctuation, etc.)\n",
        "    text = re.sub(r'[^A-Za-zÀ-ÿ\\s]', '', text)\n",
        "    text = re.sub(r'\\s+', ' ', text)  # Replace multiple spaces with one\n",
        "    return text.strip().lower()"
      ],
      "metadata": {
        "id": "4KEjvrizDZas"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "C21w6d3TvYo2"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import os\n",
        "\n",
        "def download_text(url, filename, download_dir = 'corpora'):\n",
        "    # Downloading a .txt file from Project Gurtenberg and saving it\n",
        "\n",
        "    os.makedirs(download_dir, exist_ok=True)\n",
        "    # To ensure the folder exists\n",
        "\n",
        "    response = requests.get(url)\n",
        "    if response.status_code == 200:\n",
        "      path = os.path.join(download_dir, filename)\n",
        "      with open(path, 'w', encoding = 'utf-8') as f:\n",
        "        f.write(response.text)\n",
        "      print(f\"Downloaded and saved to {path}.\")\n",
        "    else:\n",
        "      print(\"Failed to download.\")\n",
        "\n",
        "def load_corpus(filepath):\n",
        "  # Load raw text from file\n",
        "  with open(filepath, 'r', encoding = 'utf-8') as f:\n",
        "    return f.read()\n",
        "\n",
        "def save_cleaned_text(language, cleaned_text, output_dir = 'clean_corpora'):\n",
        "  # Save clean text to new file\n",
        "  os.makedirs(output_dir, exist_ok=True)\n",
        "  out_path = os.path.join(output_dir, f\"{language}.txt\")\n",
        "  with open(out_path, 'w', encoding = 'utf-8') as f:\n",
        "    f.write(cleaned_text)\n",
        "\n",
        "def process_all_languages(gutenberg_urls, output_dir = 'corpora'):\n",
        "  # Download, clean, and save corpora from URLs\n",
        "\n",
        "  download_dir = 'corpora' # Separate directory\n",
        "\n",
        "  for lang, url in gutenberg_urls.items():\n",
        "    filename = f\"{lang}.txt\"\n",
        "    download_text(url, filename, output_dir)\n",
        "    path = os.path.join(download_dir, filename)\n",
        "    raw_text = load_corpus(path)\n",
        "    cleaned = clean_text(raw_text)\n",
        "    save_cleaned_text(lang, cleaned, output_dir)\n",
        "    print(f\"Processed: {lang}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gutenberg_urls = {\n",
        "        \"english\": \"https://www.gutenberg.org/cache/epub/1342/pg1342.txt\", # Pride and Prejudice\n",
        "        \"french\": \"https://www.gutenberg.org/cache/epub/17489/pg17489.txt\", # Candide\n",
        "        \"german\": \"https://www.gutenberg.org/cache/epub/2000/pg2000.txt\", # Faust\n",
        "        \"spanish\": \"https://www.gutenberg.org/cache/epub/16172/pg16172.txt\", # Don Quixote\n",
        "        \"italian\": \"https://www.gutenberg.org/cache/epub/1012/pg1012.txt\" # Divine Comedy\n",
        "    }\n",
        "process_all_languages(gutenberg_urls)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aUlgo7wrDSTJ",
        "outputId": "c6857bb0-5386-4aae-89e1-5bbc5f346121"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloaded and saved to corpora/english.txt.\n",
            "Processed: english\n",
            "Downloaded and saved to corpora/french.txt.\n",
            "Processed: french\n",
            "Downloaded and saved to corpora/german.txt.\n",
            "Processed: german\n",
            "Downloaded and saved to corpora/spanish.txt.\n",
            "Processed: spanish\n",
            "Downloaded and saved to corpora/italian.txt.\n",
            "Processed: italian\n"
          ]
        }
      ]
    }
  ]
}